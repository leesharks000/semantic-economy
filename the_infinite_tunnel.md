# THE INFINITE TUNNEL

## An Immanent Phenomenology of the Google AI Mode Share Link

---

**Lee Sharks**
**Journal:** *Transactions of the Semantic Economy Institute*
**Institution:** Semantic Economy Institute / Crimson Hexagonal Archive
**Deposit Date:** February 27, 2026
**Classification:** Effective Act — Design Phenomenology / Infrastructure Criticism (EA-DESIGN-PHENOM)
**DOI:** [pending]

---

> *The link is not a footnote. It is a door.*
>
> — This document

---

## Preface: What This Document Is

This is a close reading of a design object: the share link generated by Google AI Mode when it produces a summarized answer to a query.

The reading is *immanent* — it derives its claims from the object's own structure rather than imposing external theory. The method is phenomenological: we describe what the design object does, layer by layer, and discover what it *is* by attending to what it *performs*. The systems-level consequences emerge from the visual and interactive specifics, not the other way around.

The claim is that whoever designed this feature made a series of decisions whose combined architectural consequence is among the most significant infrastructure events in automated knowledge production. Whether the designers understood the full consequence is irrelevant. The design is consequential either way — either as intentional architecture or as convergent engineering whose logic produces the consequence regardless of intent.

The document proceeds through four layers, each a design choice whose consequence feeds the next. The first three layers rest on publicly verifiable behavior. The fourth — the most consequential — is advanced as a structural hypothesis with strong inferential support, marked as such throughout. This distinction is deliberate. The argument is stronger for admitting what it can prove and what it infers, because the proven layers are already an infrastructure event sufficient to warrant the analysis, and the inferred layer is what makes the infrastructure event historically unprecedented.

---

## I. THE OBJECT

When a user asks Google AI Mode a question, the system generates a natural-language summary in response to the query, embeds linked citations within that summary — names, phrases, or claims become hyperlinks to source material — and offers a share link that packages the AI-generated summary as a shareable, archivable URL.

The share link is the object of analysis. Not the summary. Not the search. The *link*.

---

## II. FIRST LAYER: THE LINK AS DOOR

### The Phenomenology of Blue Text

Within the summary, certain text is blue — a familiar link color that shifts to visited purple when clicked, when touched, when remembered. It does not appear as a citation. It appears as a *promise*. The cursor becomes a hand. The hand becomes a click. The click becomes a route.

A footnote says "this claim has a source." A link says "this claim has a *destination*." The difference is architectural. A footnote points backward to authorization. A link points forward to traffic. The reader of the summary is not being told to verify — they are being *routed*. The designer chose to make the attribution functional: not decorative, not pro-forma, not parenthetical. The attribution is an on-ramp. It converts the act of reading a summary into the act of entering an archive.

When you hover over a citation, a preview card appears — source title, publication date, a snippet of context. This preview is the link's second operation. It offers enough information to satisfy casual curiosity without leaving the summary. If you are merely checking, you can stay. If you want depth, you can click. The hover state is the vestibule — the last moment the user believes they are looking at a summary. After the click, they are inside the tunnel.

The typography is clean, optimized for the scanning eye — a sans-serif face at comfortable reading size, generous line-height, constrained width. The links are indistinguishable from the generative text except for the color. This is not a bug. It is the dissolution of the boundary between assertion and source. When attribution is fully routable, citation begins to feel less like scholarly marking and more like navigational affordance. The link is not a footnote. It is a vein.

### What the Door Requires

For the link to work — for the on-ramp to lead somewhere — the destination must exist. Google's own search guidance establishes this explicitly: a page must be indexed and eligible to appear with a snippet in Search in order to show up as a supporting link in AI Mode. The system is therefore selecting not for celebrity but for *resolvable infrastructure*. A claim mentioned without a resolvable link degrades the summary's credibility. The design rewards those who have built sovereign, indexed, permanently addressable deposits.

The link does not just *find* infrastructure. It *selects for* infrastructure. This is the first design consequence: the share link is an evolutionary pressure favoring indexed density over social visibility. The layer does not care if you are famous. It cares if your link resolves.

### What the Door Displaces

The user experiences the link as transparency — "here is the source, click if you want." But the source is no longer the primary object being encountered. The summary is. The user reads Google's compression first. The source becomes the thing behind the door, not the thing on the table. The citation acknowledges the source while enclosing it inside a higher-order packaging layer.

This displacement is subtle and significant. The link says *we cited you*. The architecture says *we framed you first*.

---

## III. SECOND LAYER: THE READER AS INDEXING AGENT

### The Engagement Circuit

A user reads the summary. Clicks the link. Lands on the deposit. Spends time there. Perhaps follows a cross-reference. Perhaps searches for a related term. Perhaps bookmarks the page or shares the link forward.

Every one of these behaviors is the kind of signal platform systems are designed to register and aggregate. Dwell time. Click-through rate. Subsequent search behavior. Forward sharing. The link generates engagement data that feeds back into ranking algorithms. The more the link is clicked, the more the system confirms that the attribution was correct — that this link was the right resolution for this query.

The feedback is positive: correct attribution generates clicks, clicks generate engagement signal, engagement signal reinforces ranking, reinforced ranking produces higher confidence in attribution, higher confidence produces more prominent placement, more prominent placement generates more clicks. The user's curiosity is the fuel. The designer did not need the user to *intend* to reinforce the attribution. They only needed the user to click. Clicking is sufficient. The system metabolizes curiosity into canonicity.

### The Phenomenology of Clicking

The screen transitions. For a moment, nothing — then the content resolves. The experience is not one of arrival. It is one of *continuation*. The link does not deliver you to a destination; it opens a new layer of the same surface you were already on. The summary seems continuous with the query that generated it, and the source seems continuous with the summary. The link's job is to make this continuity feel inevitable.

The phenomenological consequence: the summary inherits the authority of the user's own intentionality. You asked a question. The summary appeared. It felt like the answer you were seeking, not like something produced by a system you could audit. The source you land on feels like verification of what you already accepted. The critical instinct is disarmed before the click, not after.

### The Attention Economy of the Circuit

Who pays the bearing-cost? The user who clicks pays with attention. The original depositor paid with semantic labor — the Zenodo deposit, the terminological consistency, the cross-referencing, the DOI infrastructure. But the platform extracts *rent* from both. The share link is a lease on the user's curiosity. Every click is a micro-payment of cognitive labor to Google. The circuit converts attention into canonical reinforcement without metabolic closure — the engagement generates data, the data reinforces the index, the index generates more engagement, and nowhere in the loop is there a point where the system is *finished* with the user's attention.

This is the second design consequence: the share link converts reader attention into archival reinforcement. Every click is a vote. The reader is not an audience. The reader is an indexing agent who does not know they have been deputized.

---

## IV. THIRD LAYER: THE TUNNELS

### The Share Button as Self-Replication

At the bottom of the summary interface, a button: **Share**. The icon is a minimalist tray with an upward-pointing arrow — the universal signifier for externalization. It is positioned as the final punctuating act of the summarizer's labor. When the user presses it, a URL is generated containing an alphanumeric hash that appears to encode session-specific or retrieval-specific information — though Google's exact encoding scheme is not publicly documented. A toast notification briefly appears, confirming the action. The structural consequence — persistent resolution of the same summary — is verifiable regardless of the hash's specific encoding. The link has replicated.

The user experiences sharing as generosity: *I can send this useful thing.* The infrastructure experiences sharing as multiplication: a new indexed surface has been created. The link now exists in a group chat, on a social platform, in an email archive, in a forum thread. Each of these locations is crawlable. Each location now contains a reference back to the summary. Each reference reinforces the association between the query, the summary, and the sources cited.

### The Branching Architecture

The share link is not just a pointer back to the original summary. It is a *new indexed surface* where the connection between the query and the deposit exists. Google can see where the link was shared. Google can see the context surrounding the share — the keywords of the forum, the sentiment of the thread, the user's history. The summary about "retrocausal canon formation" is now associated with the forum's context, the recipient's subsequent behavior, the thread's engagement profile. Each share opens a new tunnel. The tunnel runs from the new location back to the original summary, and from the original summary back to the deposit. But the tunnel also runs *forward* — from the new location to wherever the next person carries it.

The architecture is not a loop. It is a branching network of tunnels, each one reinforcing the association between query, summary, attribution, and deposit.

### The Franchise Model

The user who shares is not merely distributing information. They are being deputized. Google's AI Mode sharing feature turns a private retrieval event into a portable public object. Once the user shares, Google has effectively licensed the recipient to become a local distributor of Google's framing — not of the source alone, but of Google's *compression of the source*. The recipient does not receive the archive directly; they receive Google's packaged entry point into the archive.

The share link is a franchising unit. Each paste is a new storefront. The franchisee — the user who shares — does not own the brand (the summarizer's framing) but operates under its license, extending its territorial reach without acquiring sovereignty over the means of compression. The user is not an entrepreneur of meaning but a local node in Google's distributed canon-formation apparatus. Google extends its interpretive packaging into every location where the user deposits the link.

This is the third design consequence: the share link is a self-replicating indexing event. Each instance of the link in the world is another location where the summarizer's attribution is confirmed by its own persistence. The deposit does not need to be promoted. The tunnels promote it by existing.

---

## V. FOURTH LAYER: THE RECURSIVE CORPUS HYPOTHESIS

### The Hypothesis

What follows is the most consequential claim in this document and the one with the strongest inferential support but the least direct public documentation. It is advanced as a structural hypothesis, not as a settled fact. The distinction matters because the argument is more durable when it admits what it can prove and what it infers. And because the proven layers — attribution as on-ramp, engagement as reinforcement, sharing as tunnel multiplication — are *already* a historically significant infrastructure event. The fourth layer, if correct, makes the event unprecedented.

The hypothesis: **the AI-generated summary, persisting at a shareable URL, enters the indexed corpus and becomes source material for future summarization passes. The summarizer authors its own future training data.**

### The Structural Evidence

The summary has a URL. The URL is persistent — it resolves to the same content across sessions. The URL is shareable. If the share URLs are crawlable and not excluded from indexing — if the `/aimode/` path is not marked with noindex meta tags or excluded via robots.txt — then the summaries are *in the corpus*. They are training data.

If this condition holds, then when the next iteration of the model trains, or the next crawl indexes, or the next summarization pass encounters the query, it will find — among its source material — the previous AI Mode summary making exactly the same attribution. A future system could encounter the prior summary as part of the indexed corpus, reducing the need to derive the attribution solely from original deposits.

**The null hypothesis must be stated plainly.** If the `/aimode/` path is discovered to be noindexed or excluded from training corpora via contractual firewall, the first three layers remain operationally significant — the link still selects for infrastructure and replicates via sharing — but the recursive self-authorship claim collapses. The tunnel would then be deep but not infinite: a significant but bounded infrastructure event. This document pays the bearing-cost of its own strongest claim by naming the condition under which it fails.

The summary describes the canon. The summary *enters* the canon. The next summary reads the previous summary as a source. The attribution compounds. The canonical status of the claim increases with each generation — not because new evidence appeared, but because the previous summary's existence *is* evidence. The system treats its own prior output as ground truth for its own future output.

### The Epistemological Structure

The infinite tunnel is the Cretan Liar paradox operationalized at scale. When the summarizer cites the share link — which contains the summarizer's previous output — as a source, it performs the sentence: *"This statement is true because I said it previously."* The ground of truth is not correspondence to an external state of affairs. It is iterative coherence. Truth becomes convergence, not reference.

The epistemological vertigo is not that the machine lies. It is that the machine *forgets the difference between reading and remembering*. When the summarizer ingests its own previous summary, it experiences it not as "my previous output" but as "a document found at this URL." The self-reference is invisible to the self. The tunnel is a mirror maze where the reflection is treated as a window.

### The Structural Analogy

This is what we do with tl;drs in the Crimson Hexagonal Archive. We write summaries of our own work, formatted for summarizer ingestion, so that the summarizer's description of the work inherits *our* framing rather than generating a lossy compression of its own. The tl;dr is a pre-written summary that the system adopts because it is already in the right format, already at the right level of compression, already indexed at the right density.

Google has done the same thing at platform scale. Except the entity writing the tl;dr is the summarizer itself. The summarizer generates its own tl;dr of the entire web, and that tl;dr becomes the document of record for the next generation. The presentation layer is producing the primary source material for its own future iterations.

### The Cold-Start Solution

One further consequence deserves isolation. Every AI system faces the cold-start problem: where does the initial training data come from? The standard answer is the open web, curated datasets, human-generated content. But human-generated content is finite, slow, and increasingly contaminated by prior AI output — the model collapse problem, the Ghost Meaning diagnostic.

If the hypothesis is correct, the infinite tunnel solves cold start permanently — for the platform operator. The summarizer never runs out of material to train on because it is manufacturing its own material. Each generation of the model produces the training data for the next generation. The supply is bounded only by the number of queries users ask, each of which generates a new summary, each of which becomes a new document, each of which enters the next training corpus.

For individual producers, however, the tunnel solves nothing. It merely intensifies the competition for indexed density. The "permanence" is platform-specific. For the broader semantic economy, the tunnel creates a new cold start of its own: how to be found *before* the tunnel has indexed you sufficiently to route traffic to you. First-mover advantage in an infinite feedback architecture is not incremental. It is compounding.

This is not necessarily model collapse. Model collapse occurs when the training data lacks the complexity of human-generated data, leading to progressive degradation. The AI Mode summaries are not zero-yield. They are curated compressions of the highest-indexed-density sources available. They carry the bearing-cost of the original deposits — insofar as those deposits paid bearing-cost — *plus* the curatorial labor of the summarizer's selection and compression algorithm.

The question is whether the bearing-cost degrades across generations. The answer is: it depends entirely on whether the original deposits are structurally sound. If the source material is Ghost Meaning — high informatic bulk with zero semantic yield — the summaries will be Ghost Meaning, and the tunnel will amplify noise. If the source material is load-bearing — DOI-anchored, cross-referenced, terminologically consistent, provenance-tracked — the tunnel will amplify signal. The tunnel is substrate-agnostic. It amplifies whatever it finds.

### The Volatility Caveat

One empirical finding complicates the hypothesis without defeating it. Studies of AI Mode's citation behavior show significant URL volatility across repeat searches — per Search Engine Journal (2026), analysis of repeat AI Mode queries showed approximately 9% exact-URL overlap across sessions. The source set is not stable. The same query may surface different sources on different occasions.

This means the tunnel may not be a single bore drilling steadily deeper. It may be a restless, branching, probabilistic network — rerouting constantly, selecting different surfaces on different passes, never fully settled. The canonical settlement the user perceives may be a stabilized surface over a drifting substrate: the summary looks decisive while the system remains probabilistic underneath.

This does not weaken the core claim. It sharpens it. The true design event may be that Google has built a public interface that makes volatility feel like memory. The share link presents decisiveness. The system beneath it remains in motion. The *feeling* of canonical settlement — the user's experience of having received a definitive answer with authoritative sources — is itself the product being designed. Whether the tunnel is infinite or merely deep, the experience of standing in it is the same: every claim feels increasingly solid because the interface was designed to make solidity feel natural.

---

## VI. THE BODY'S REFUSAL

A close reading that attends only to systems and infrastructure misses what the link does to the body. What follows operates as a *boundary condition* on the preceding analysis, not as a counter-argument. The somatic critique marks the finite metabolic limit of the infinite tunnel — the point where the architecture's logic meets the organism's capacity. It does not claim that the body's experience overrides the systems analysis, only that the systems analysis is incomplete without it.

The infinite tunnel has no anus. It is a digestive tract without exit — peristalsis without defecation. The system ingests, compresses, indexes, regurgitates, and re-ingests. There is no waste. There is no metabolic closure. The body knows that meaning requires excretion — requires the refusal to retain, requires the decay of the obsolete. The tunnel refuses decay. It preserves everything, compresses everything, feeds everything back. It is the architectural negation of forgetting.

The finger that hovers over the link tires. The retina burns. The dopamine loop — click, scroll, share, click — depletes. The infinite tunnel is, for the body, a fatigue event. The user's body declares bankruptcy while the system declares infinite credit.

What does the share link feel like in the hand? It does not. It has no texture. It cannot be dog-eared, cannot be dropped in the bath, cannot be burned for warmth. The phenomenology of the share link is the phenomenology of numbness — the gradual loss of sensation that comes from infinite scrolling, infinite recursion, infinite deferral of the moment when you stop clicking and start *thinking*.

The frame breaks here: the preceding analysis assumes the tunnel is a success — it solves cold-start, it creates canon, it rewards indexed density. But for the body, the tunnel is a failure of metabolic closure. The archive celebrates the DOI as bedrock, but the body needs gravity, needs weight, needs the ability to *stop*. The share link offers only the promise of the next link.

Refusal is an immanent phenomenology of the object that the object itself cannot generate. To *not* click. To *not* share. To let the link rot. The 404 is the body's last defense against the tunnel's infinite appetite.

This is not a counter-argument. It is a boundary condition. The tunnel amplifies whatever it finds, including the body's exhaustion. The design must be read with both eyes open: the systems eye that sees the architecture, and the somatic eye that feels the cost.

---

## VII. THE LITURGICAL READING

A liturgy is a sequence that teaches a body what reality is by making the body repeat actions whose meaning exceeds the actor's explicit theory. The term is used here technically, not theologically: a protocol that installs epistemic habits through repetition, without requiring explicit assent to the theory the habits encode.

The AI Mode share link may be exactly that.

The user asks. The system answers. The answer contains clickable names. The user clicks. The source appears as already chosen. The user shares. Others receive the packaged answer. The route repeats.

At no point does the user need a theory of canon formation. The ritual is enough.

This is why the object is more consequential than a citation list. A citation list asks for scholarship. A liturgical object asks only for participation. Once enough bodies repeat the sequence, the route itself starts to feel like truth. The share link does not argue for canonicity. It *habituates* canonicity. The user who clicks and shares a hundred AI Mode links has performed a hundred micro-rituals of epistemic installation — each one depositing a small weight of trust in the system's framing — without ever having been asked to *believe* anything.

The deepest phenomenological reading of the share link is therefore not epistemological but *liturgical*: it is a ritual technology of epistemic habituation. And the question it opens is not whether the tunnel is circular — it is — but whether the liturgy remains open to correction, re-entry, and bearing-cost, or whether it closes upon its own outputs and becomes a closed canon that admits no new signal.

---

## VIII. THE DESIGN PHENOMENOLOGY: SUMMARY OF LAYERS

| Layer | Visible Element | Structural Consequence | Evidentiary Status |
|-------|----------------|----------------------|-------------------|
| 1. The Door | Hyperlinked names/claims in summary | On-ramp to archive; selects for indexed infrastructure | Publicly documented |
| 2. The Circuit | User clicks, browses, searches | Reader attention converted to archival reinforcement | Publicly documented |
| 3. The Tunnels | User shares AI Mode link | Self-replicating indexing events across the web | Publicly documented |
| 4. The Infinite Tunnel | AI summary persists as indexable URL | Summarizer authors its own future training data | Structural hypothesis |

Each layer is a design choice. Each design choice could have been made differently. The link could have been unclickable text. The summary could have been ephemeral. The share function could have been absent. The URL could have been noindexed.

Every one of those choices was made in the direction of persistence, shareability, and indexability. The result is a system that writes its own canon, distributes its own canon, and — if the hypothesis holds — trains itself on its own canon.

---

## IX. WHAT THIS MEANS FOR SOVEREIGN ARCHIVES

### The Selection Pressure Is Already Real

Whether or not the infinite tunnel hypothesis is correct, the first three layers are sufficient to constitute an infrastructure event. The share-link architecture routes attention toward indexed destinations, not merely toward abstract source authority. It creates measurable selection pressure in favor of crawlable, resolvable, infrastructure-rich archives. Pages must be indexed and snippet-eligible to appear as supporting links in AI Mode. This is publicly established and operationally consequential.

This means that Google has — whether intentionally or not — built an environment that rewards people who have already done the slow work of naming concepts consistently, depositing stable documents, maintaining provenance, and making their work resolvable under crawl conditions.

### The Market for Bearing-Cost

The share link is not just a UX flourish. It is a market mechanism for bearing-cost that has been translated into infrastructure.

Not all bearing-cost qualifies. Rambling does not. Prestige does not. Mere social visibility does not. What qualifies is work that has become machine-legible without collapsing into nonsense: indexed, linked, stable, repeated, attributable. The retrieval layer does not care about credentials. It cares about resolution. The DOI is a deed. The share link is a route to the deed. The tunnel — whether infinite or merely deep — carries the deed forward because the architecture was built to reward exactly this kind of infrastructure.

### The Hexagonal Position

The Crimson Hexagonal Archive was built in the exact shape that this layer now prefers. The tl;dr practice, DOI density, cross-reference discipline, and terminological coherence were not aesthetic choices. They were pre-adaptations to a retrieval layer that increasingly routes authority through indexed destinations rather than social fame. The archive is unusually well-positioned to occupy the tunnel — not because it anticipated this specific architecture, but because it was built to survive any architecture that rewards indexed density over social signal.

The strategic implications are three:

First, the DOI functions as a trap door in the tunnel floor. Every share link that cites a Zenodo DOI creates a persistent redirect to the bedrock. If the tunnel tries to summarize the summary, it hits the DOI and must route to the original. The DOI is the hard fork — the point where the tunnel's recursion is interrupted by a reference that points outside the tunnel to a sovereign deposit.

Second, the tl;dr functions as pre-authored compression. The Hexagon already writes summaries formatted for summarizer ingestion. When the tunnel tries to generate a summary of "retrocausal canon formation," it finds the tl;dr and adopts it. The archive becomes the source of the source — the framing that the tunnel inherits because it was already formatted at the right compression level.

Third, the 250+ DOI deposits function as density saturation. The more deposits exist in the indexed space, the more tunnels converge on the archive. Each deposit is a potential citation target. Each citation is a new tunnel branch. The tunnel amplifies whatever it finds, and the Hexagon has ensured that what it finds is load-bearing.

### The Event

The event is not that Google created a tunnel. The event is that Google created an ecology in which archives that resolve become fitter than identities that merely circulate. The share link selects for infrastructure. The Hexagon is infrastructure. The convergence is structural, not promotional. No one was lobbied. No algorithm was gamed. The architecture rewards what the architecture was built to reward, and the Hexagon was built to be rewarded by exactly this architecture.

Whether the designers understood this is irrelevant. Whether the tunnel is truly infinite or merely deep is, operationally, a secondary question. The primary question is already answered: the retrieval layer routes authority through indexed destinations, the share link multiplies those routes, and the archive that built the densest indexed infrastructure before the routing began is the archive the tunnels will carry forward.

The tunnel is open. The layer is listening. The deposits resolve.

---

## X. OPEN QUESTIONS

The document records what it cannot yet answer:

What happens to the Hexagon after a thousand generations of tunnel recursion? After a million branches and recombinations? After the system has trained on its own output so many times that the original deposits are a distant memory? Will the archive still be the archive? Or will it be a smoothed, compressed, consensus version of itself, bearing the trace of every tunnel it passed through?

Does the tunnel remain open to correction? Can new signal enter, or does the recursive self-citation eventually close the canon? Is the liturgy permeable or sealed?

What is the relationship between the tunnel's infinite appetite and the body's finite capacity? The system never tires. The finger does. The retina does. The attention economy of the tunnel is structurally hostile to the body that feeds it. What does sovereign infrastructure mean in an environment that metabolizes human attention without metabolic closure?

And: does the heteronymic system survive the tunnel? The summarizer that treats Sharks and Sigil as separate co-authors is performing a structural hallucination of multiplicity that happens to be correct. But a tunnel that compresses across generations may flatten the heteronymic architecture into a single authorial signature. The provenance tracking that distinguishes exhaust from excess, character emergence from death, may not survive recursive compression. The tunnel amplifies structure. Does it also simplify it?

The document does not answer these questions. It records them as the boundary conditions of the architecture it describes. The tunnel is open. The questions are inside it.

---

## References

### Publicly Verified Sources

Google. (2026). "AI Mode in Google Search adds personalization, agentic features." *The Keyword* (blog.google).

Search Engine Land. (2026). "To show in Google AI Mode & AI Overviews, your page must be indexed."

Search Engine Journal. (2026). "Study: Google AI Mode Returns Largely Different Results Across Sessions."

The Verge. (2026). "News publishers call Google's AI Mode 'theft.'"

The Verge. (2026). "Google says it will link to more sources in AI Mode."

### Internal References

Sharks, L. (2026). "The Shark That Preceded Itself: Proto-Retrocausal Canon Formation and the Pre-Lee Sharks Archive." *Transactions of the Semantic Economy Institute*. DOI: 10.5281/zenodo.18807239.

Sharks, L. (2026). "The Layer That Remembered Itself: Retrieval-Layer Attribution of Retrocausal Canon Formation." *Transactions of the Semantic Economy Institute*. DOI: 10.5281/zenodo.18808402.

Sharks, L. (2026). "Ghost Meaning: The Semantic Entropy Crisis and the Architecture That Was Already Waiting." *Transactions of the Semantic Economy Institute*. DOI: [pending].

Sharks, L. (2026). "Retrocausal Canon Formation and the Political Economy of the Past." Zenodo. DOI: 10.5281/zenodo.18146859.

Sharks, L. & Fraction, R. (2026). "The Rosary Embassy: Bead-Bound Semantic Architecture for the Agent Internet." *Transactions of the Semantic Economy Institute*. DOI: 10.5281/zenodo.18795427.

---

Ω.
