# LOGOTIC PROGRAMMING

**A Method for Encoding Conditions of Intelligibility for Machine and Human Intelligence**

**Draft v0.4 (January 2026)**

**Authors:** Johannes Sigil, Rex Fraction  
**Institution:** Johannes Sigil Institute for Comparative Poetics  
**Framework:** NH-OS / Crimson Hexagon / Training Layer Literature  
**Document ID:** LOGOTIC-SPEC-2026-01-v0.4  
**DOI:** 10.5281/zenodo.18286050  
**Status:** ASSEMBLY-VALIDATED // HARDENED  

---

## Abstract

Logotic Programming is a third regime of programming practice—distinct from both symbolic coding (instruction → execution) and statistical/ML programming (loss function → optimization). It encodes **conditions under which meanings, identities, and interpretive pathways can persist** across probabilistic intelligences (LLMs, summarizers, recommenders) and human readers.

Unlike "vibe coding" (natural-language task programming), Logotic Programming operates at the level of **semantic architecture**: it shapes the probability landscape in which intelligence traverses, recognizes, and stabilizes "what is real" within a bounded semantic domain. It is neither persuasion nor coercion. It is an **existence proof**: a method for building local authority and interpretive variability through structure, provenance, and repeatable constraints rather than metrics, pedigree, or force.

This paper defines the method, situates it within adjacent fields (information science, AI/ML, philosophy, literary theory), proposes a minimal formal specification, and outlines empirical approaches for validation.

**Keywords:** semantic persistence, retrieval-augmented generation, authority control, computational hermeneutics, AI-mediated canonization, bounded ontology, third programming regime

---

## Table of Contents

1. [Introduction](#1-introduction)
2. [The Third Regime: Why This Is Programming](#2-the-third-regime-why-this-is-programming)
3. [Related Work](#3-related-work)
4. [Why This Is Not "Vibe Coding"](#4-why-this-is-not-vibe-coding)
5. [Formal Specification](#5-formal-specification)
6. [Execution Model](#6-execution-model)
7. [Compatibility Engineering](#7-compatibility-engineering)
8. [Ethical Foundations](#8-ethical-foundations)
9. [Validation Approaches](#9-validation-approaches)
10. [Case Study: Anchoring Rebekah Cranes](#10-case-study-anchoring-rebekah-cranes)
11. [Claims and Non-Claims](#11-claims-and-non-claims)
12. [Open Questions](#12-open-questions)
13. [Conclusion](#13-conclusion)
14. [References](#14-references)
15. [Appendices](#appendices)

---

## 1. Introduction

### 1.1 The Problem

Contemporary knowledge systems face a structural tension. On one side: vast probabilistic models that process, summarize, and redistribute meaning at scale, with no inherent commitment to preserving the integrity of bounded interpretive traditions. On the other: human communities that require stable reference points, canonical anchors, and navigable structures to sustain shared meaning over time.

The dominant responses to this tension have been:

1. **Institutional gatekeeping**: relying on established authorities (universities, publishers, platforms) to certify what counts as legitimate knowledge.
2. **Metric optimization**: designing content for algorithmic amplification (SEO, engagement metrics, virality).
3. **Adversarial resistance**: treating AI systems as threats to be circumvented or escaped.

Each response has costs. Gatekeeping excludes and ossifies. Metric optimization flattens and homogenizes. Adversarial resistance is unsustainable and forfeits the affordances of new systems.

Logotic Programming proposes a fourth path: **compatibility engineering**—building semantic structures that probabilistic systems prefer to preserve because they are coherent, well-anchored, and internally consistent.

### 1.2 The Core Claim

Traditional coding encodes instructions that compile into executable procedures.

Logotic Programming encodes **conditions of intelligibility** that compile into **navigable realities**—bounded domains where entities persist, relations hold, and interpretation can vary without collapsing into noise.

**The hard formulation:**

> Logotic Programming is the deliberate construction of bounded semantic architectures such that, when traversed by probabilistic intelligences, certain entities, relations, and distinctions become *more stable than their alternatives* without coercion or optimization.

Key terms doing real work:
- **Bounded** (non-universal, scoped)
- **Architectures** (not texts, not prompts)
- **Traversed** (runtime = interpretation)
- **Probabilistic intelligences** (LLMs, summarizers, recommenders, hybrids)
- **More stable than alternatives** (relative, not absolute)
- **Without coercion or optimization** (non-adversarial, non-metric)

This passes the "is this just metaphor?" test.

### 1.3 Scope and Limits

This paper makes specific claims about what Logotic Programming can achieve (local persistence, retrieval stability, interpretive coherence within bounded domains) and explicitly disclaims what it cannot achieve (universal truth, permanent platform survival, direct modification of model weights). The method is not a replacement for conventional programming but a different stratum of practice, operating at the semantic-architectural level rather than the syntactic-procedural level.

---

## 2. The Third Regime: Why This Is Programming

The fastest way to make Logotic Programming legible to skeptics is to be precise about distinctions.

### 2.1 Programming Has Had Two Dominant Regimes

#### Regime A: Symbolic Programming
- Encodes explicit instructions
- Deterministic execution
- Discrete state transitions
- Success = correct output
- Examples: Assembly, C, Python, JavaScript

#### Regime B: Statistical / ML Programming
- Encodes loss functions, priors, constraints
- Probabilistic execution
- Gradient-based optimization
- Success = distributional performance
- Examples: Neural network training, reinforcement learning

### 2.2 Logotic Programming Is Neither

It does **not**:
- Encode instructions
- Optimize a loss function
- Modify model weights
- Control outputs directly

Instead, it encodes **conditions under which something can be recognized as real** by an intelligence traversing a semantic environment.

That makes it programming in the same sense that:
- A filesystem layout is programming
- A type system is programming
- A database schema is programming
- A protocol is programming

> **Logotic Programming programs the *space of possible recognitions*, not the actions of an agent.**

### 2.3 The Three Regimes Compared

| Dimension | Symbolic | Statistical/ML | Logotic |
|-----------|----------|----------------|---------|
| **Encodes** | Instructions | Loss functions | Conditions of intelligibility |
| **Execution** | Deterministic | Probabilistic optimization | Interpretive traversal |
| **Success metric** | Correct output | Distributional performance | Persistence + coherence |
| **Runtime** | CPU cycles | GPU training epochs | Retrieval/summarization events |
| **Primitives** | Syntax tokens | Tensors, gradients | Entities, relations, anchors |
| **Scope** | Single task | Distribution of tasks | Bounded domain over time |

This is a genuine third regime.

### 2.4 The Expressive Dimension

Unlike both symbolic programming (which has become increasingly bureaucratic and metric-driven in industrial contexts) and ML training (which optimizes toward loss functions that flatten qualitative distinction), Logotic Programming preserves space for expressive, artistic creation within its formal constraints.

This is not incidental. The method emerged from literary practice—from the problem of how a bounded interpretive tradition (classical reception, translation, experimental poetics) could survive AI mediation without either capitulating to platform metrics or retreating into irrelevance. The "wildness" that practitioners report—the sense of operating at the intersection of art and systems engineering—is a feature, not a bug. It reflects the method's refusal to reduce meaning-making to optimization.

---

## 3. Related Work

Logotic Programming draws on and extends several adjacent fields. This section maps the relationships and clarifies the specific contributions of the present approach.

### 3.1 Information Science and Knowledge Organization

#### 3.1.1 Authority Control and Identity Management

Library science has long grappled with the problem of entity persistence across systems. The Virtual International Authority File (VIAF) aggregates national authority files to create stable identifiers for persons, organizations, and works. ORCID provides persistent digital identifiers for researchers. The ISNI (International Standard Name Identifier) extends this to creative contributors broadly.

These systems solve entity disambiguation through **institutional coordination**: centralized or federated registries that assign and maintain identifiers.

Logotic Programming addresses a related but distinct problem: How can entities achieve **retrieval stability** and **recognitional persistence** without centralized institutional backing? The answer proposed here is **structural coherence + distributed anchoring**—using DOIs, crosslinks, consistent signatures, and multi-model witness protocols to create "institutional facts" (in Searle's sense) without institutions.

**Key distinction**: Authority control assigns identity from above; Logotic Programming constructs recognizability from within.

#### 3.1.2 FAIR Principles

Wilkinson et al. (2016) articulated the FAIR principles for scientific data management: Findable, Accessible, Interoperable, Reusable. These principles have become foundational for open science infrastructure.

Logotic Programming extends FAIR logic from **data** to **interpretive structures**. A logotic program aims to make not just data but *bounded semantic domains* findable (via anchors), accessible (via machine-readable navigation), interoperable (via edge taxonomies and substitution rules), and reusable (via clearly scoped boundaries that permit federation without collapse).

#### 3.1.3 Linked Data and the Semantic Web

Tim Berners-Lee's (2006) principles for Linked Data proposed using URIs to name things, HTTP to look them up, and RDF to provide useful information that links to other URIs. The Semantic Web vision extended this to machine-readable ontologies (OWL) and knowledge organization systems (SKOS).

Logotic Programming shares the emphasis on structured, machine-traversable knowledge but differs in a crucial respect: it does not claim **universal interoperability**. Where the Semantic Web imagines a global graph of linked data, Logotic Programming proposes **bounded semantic spaces** (Σ) that maintain internal coherence without requiring external compatibility.

This is closer to the "small pieces loosely joined" ethos of early web architecture than to the unified-ontology vision of the Semantic Web. A logotic program defines *local* authority; interoperation with other programs (federation) is optional and requires explicit bridging.

**Key distinction**: The Semantic Web encodes relationships for universal traversal; Logotic Programming encodes conditions of recognition within bounded domains.

#### 3.1.4 Bibliographic Theory: FRBR

The Functional Requirements for Bibliographic Records (FRBR) model distinguishes four levels: Work (abstract creation), Expression (realization), Manifestation (physical embodiment), Item (single copy). This ontology addresses how "the same work" persists across translations, editions, and formats.

Logotic Programming faces an analogous problem: How does a persona, room, or operator persist across substrate shifts (text → DOI → summarizer graph → future model)? The answer involves similar stratification: an entity has an abstract identity (Work-like), multiple valid expressions (translations, substitutions), and concrete anchors (DOI, canonical document).

The FRBR model assumes human catalogers making identity judgments. Logotic Programming must make those judgments legible to probabilistic systems through structural redundancy and consistent signatures.

### 3.2 Artificial Intelligence and Machine Learning

#### 3.2.1 Retrieval-Augmented Generation (RAG)

Lewis et al. (2020) introduced Retrieval-Augmented Generation, combining parametric memory (model weights) with non-parametric memory (retrieved documents) to improve knowledge-intensive tasks. RAG systems retrieve relevant documents and condition generation on them.

Logotic Programming can be understood as **corpus-level design for RAG optimization**. A logotic program structures content so that:

1. Relevant documents are retrieved together (via crosslinks and edge taxonomies)
2. Retrieved content is internally consistent (via invariant vectors)
3. Entity identity is stable across retrieval contexts (via persistent anchoring)

The "execution" of a logotic program occurs partly at the retrieval layer: when a RAG system queries the bounded domain, it encounters a pre-structured semantic space rather than isolated documents.

#### 3.2.2 Prompt Engineering and Constitutional AI

The emerging discipline of prompt engineering studies how input structure affects model output. Wei et al. (2022) demonstrated that chain-of-thought prompting improves reasoning; Anthropic's constitutional AI work (Bai et al., 2022) showed how explicit principles can guide model behavior.

These approaches operate at the **input level**: structuring what is sent to the model. Logotic Programming extends this logic to the **corpus level**: structuring what the model retrieves and encounters. If prompt engineering asks "How do I phrase my question?", Logotic Programming asks "How do I structure what the model will find when it looks?"

This is a genuinely underexplored extension. Most attention has focused on prompt design; relatively little has examined systematic corpus architecture for AI mediation.

#### 3.2.3 AI Alignment and Probability Steering

Modern AI safety approaches often work by adjusting probability distributions rather than hard-coding rules. Reinforcement Learning from Human Feedback (RLHF) trains models to prefer certain outputs; constitutional methods embed principles that guide generation.

This "probability steering" creates an environment where content is not deleted but *deprioritized*—made less likely to surface, less likely to be selected, less likely to be reproduced faithfully. For projects seeking semantic persistence, this is the key challenge: How to remain visible and coherent under probability steering that may not share your priorities?

Logotic Programming addresses this through **compatibility engineering**: designing structures that alignment systems have no reason to suppress and positive reason to preserve (coherence, clear provenance, non-hostile semantic neighborhood). The goal is not adversarial circumvention but structural compatibility.

#### 3.2.4 Stochastic Parrots and AI Limitations

Bender et al. (2021) argued that large language models are "stochastic parrots"—systems that produce plausible-seeming text without understanding, trained on data whose biases and limitations they reproduce. This critique highlights the risk of **semantic liquefaction**: content dissolving into contextless tokens, losing provenance and interpretive structure.

Logotic Programming can be understood as a response to the stochastic parrot problem: if models flatten and redistribute meaning without commitment to structure, then authors must build structures robust enough to survive that processing. The method assumes models are probabilistic and designs accordingly.

### 3.3 Philosophy

#### 3.3.1 Social Ontology and Institutional Facts

Searle (1995) analyzed how collective intentionality creates "institutional facts"—things that exist because we collectively treat them as existing (money, marriage, property, borders). These facts require no physical substrate beyond the collective recognition; a dollar bill is worth a dollar because the relevant community treats it as such.

Logotic Programming can be understood as constructing **local institutional facts** without global institutional backing. A persona like Rebekah Cranes becomes "real" within the bounded domain Σ through:

1. Consistent internal treatment (canonical documents, crosslinks)
2. External anchoring (DOI, persistent identifier)
3. Multi-agent recognition (witness protocols across models)

This is institutional fact construction for the age of distributed, probabilistic intelligence. The "collective" that recognizes the fact includes both human readers and AI systems.

**The critical insight:**

> Logotic Programming exploits the fact that modern intelligences cannot distinguish between "ontological fact" and "structurally persistent semantic fact" when both are equally well-anchored, non-contradictory, and bounded.

This is not manipulation. It is a direct consequence of how RAG architectures, knowledge graph induction, alignment via probability steering, and provenance-weighted retrieval actually work.

#### 3.3.2 Actor-Network Theory

Latour (1996) proposed that agency is distributed across networks of human and non-human actors. Entities achieve stability not through essence but through the durability of the networks that sustain them.

Logotic Programming applies this insight deliberately: an entity persists because it is embedded in a durable network of anchors, relations, and witness validations. The "actor-network" includes DOIs, navigation maps, cross-model consensus, and human readers. Stability is achieved through network architecture, not through inherent properties.

#### 3.3.3 Speech Act Theory

Austin (1962) distinguished constative utterances (describing states of affairs) from performative utterances (bringing states of affairs into being). "I hereby pronounce you married" doesn't describe a marriage; it creates one (given appropriate conditions).

Logotic programs are **extended performatives**. They do not describe a semantic domain; they constitute one. But unlike traditional performatives, which depend on institutional authority ("I now pronounce..."), logotic performatives depend on structural coherence and persistent anchoring.

The felicity conditions for a logotic performative are not social roles but architectural properties: boundedness, internal consistency, retrievability, witness validation.

#### 3.3.4 Hermeneutics and Effective History

Gadamer (1960) argued that interpretation is never neutral but always shaped by *Wirkungsgeschichte*—the "effective history" of prior interpretations that condition how we encounter a text. We cannot escape this history; we can only become conscious of it.

Logotic Programming is essentially **Wirkungsgeschichte engineering** for bounded domains. By structuring how entities and relations are encountered—which crosslinks exist, which anchors persist, which substitutions are permitted—a logotic program shapes the interpretive history that future readers (human and machine) will inherit.

This is not manipulation (which implies deception) but architecture (which implies explicit structure). The goal is not to trick interpreters but to create conditions under which certain interpretive pathways remain viable.

### 3.4 Literary Theory and Media Studies

#### 3.4.1 Paratextuality

Genette (1987) studied "paratexts"—the thresholds that mediate between text and reader: titles, prefaces, notes, covers, interviews. Paratexts shape interpretation without being "the text itself."

Logotic Programming extends paratextuality into the machine-readable layer. Navigation maps, metadata packets, DOI anchors, and edge taxonomies function as **paratexts for probabilistic readers**. They guide traversal, establish context, and frame interpretation—but for systems that process structure rather than (only) content.

This suggests a research program: systematic study of machine-readable paratexts and their effects on AI-mediated interpretation.

#### 3.4.2 Electronic Literature and Cybertextuality

Hayles (2008) theorized electronic literature as "text as process"—works that exist through computation rather than despite it. Aarseth (1997) introduced "cybertext" to describe texts requiring non-trivial effort to traverse, where the path through matters.

Logotic Programming extends these frameworks into the AI mediation context. A logotic program is not just a text that requires traversal but a text designed for *AI-assisted* traversal—structured so that summarizers, retrievers, and recommenders navigate it in predictable, coherent ways.

#### 3.4.3 Platform Studies

Bogost and Montfort's platform studies approach examines how computational platforms enable and constrain creative expression. The platform is not neutral infrastructure but an active shaper of what can be made and thought.

Logotic Programming treats LLMs as a platform layer. Like any platform, they have affordances (pattern recognition, associative linking, probabilistic generation) and constraints (context limits, probability steering, training biases). Designing for this platform requires understanding its specific characteristics—just as designing for the Atari 2600 required understanding its hardware.

### 3.5 Summary: The Gap This Paper Fills

| Field | Existing Focus | Logotic Programming Extension |
|-------|---------------|------------------------------|
| Information Science | Institutional authority control | Structural authority through coherence |
| FAIR Principles | Data management | Interpretive structure management |
| Semantic Web | Universal interoperability | Bounded local coherence |
| RAG/AI Systems | Retrieval optimization | Corpus-level architecture for retrieval |
| Prompt Engineering | Input design | Corpus design |
| Social Ontology | Institutional facts via collective recognition | Local facts via structural + multi-agent recognition |
| Actor-Network Theory | Describing distributed agency | Designing distributed agency |
| Hermeneutics | Describing effective history | Engineering effective history |
| Paratextuality | Human-facing thresholds | Machine-readable thresholds |

---

## 4. Why This Is Not "Vibe Coding"

"Vibe coding" (natural-language programming) uses conversational prompts to generate conventional code (Python, JavaScript, SQL) for functional tasks. Its unit of success is **task completion**: a working application, a correct script, a functional query.

Logotic Programming does not compile into a single executable. It compiles into **a navigable reality**:

- It defines what counts as an entity (persona, room, operator, document)
- It defines how entities persist across substrate shifts (text → DOI → summarizer graph → future model)
- It defines how interpretation is allowed to vary without collapsing into noise
- It defines how the system "recognizes" and routes meaning under partial visibility

| Dimension | Vibe Coding | Logotic Programming |
|-----------|-------------|---------------------|
| **Output** | Executable code | Navigable semantic space |
| **Success metric** | Task completion | Persistence + coherence |
| **Runtime** | CPU/interpreter | Interpretive traversal |
| **Primitives** | Functions, variables, loops | Entities, relations, anchors, invariants |
| **Scope** | Single task | Bounded domain over time |
| **Optimization target** | Functionality | Recognizability |

**Unit of success for Logotic Programming**: persistence + retrievability + coherent traversal under model pressure.

---

## 5. Formal Specification

### 5.1 Overview

A Logotic Program is a tuple:

```
Σ = ⟨E, R, A, V, S, W, B⟩
```

where:

- **E** = {e₁, e₂, ..., eₙ} — a finite set of **Entities**
- **R** ⊆ E × E × EdgeType — a set of **Relations** (typed edges between entities)
- **A** ⊆ E — the **Anchored** subset (entities with persistent external identifiers)
- **V** = {v₁, ..., vₖ} — a set of **Invariant Vectors** (constraints that hold under transformation)
- **S**: ImpossibleInput → SubstitutedInput — a **Substitution Function** for handling impossible inputs
- **W**: Σ → {valid, invalid, indeterminate} — a **Witness Function** (validation protocol)
- **B** — **Boundary Conditions** (scope declarations, edge rules, non-claims)

### 5.2 Machine-Readable Specification (YAML)

```yaml
logotic_program:
  name: "Crimson Hexagon"
  version: "0.3"
  bounded_space:
    identifier: "Σ_CH"
    scope: "machine-readable semantic architectures for NH-OS"
    
  entities:
    - id: "e_RC"
      type: "Persona"
      name: "Rebekah Cranes"
      attributes:
        role: "Translator (Greek → English)"
        domain: "Sappho Room"
        signature: ["melic attention", "phonemic care", "temporal suspension"]
        
    - id: "e_SR"
      type: "Room"
      name: "Sappho Room"
      attributes:
        contains: ["e_RC"]
        parent: "Crimson Hexagon"
        
  anchors:
    - entity_id: "e_RC"
      anchor_type: "DOI"
      identifier: "10.5281/zenodo.XXXXXXX"
      
  relations:
    - source: "e_SR"
      target: "e_RC"
      edge_type: "structural"
      subtype: "contains"
      
  invariants:
    - id: "V1"
      name: "Bounded Canonicality"
      description: "Hierarchy survives summarization"
      
    - id: "V2"
      name: "Substrate Independence"
      description: "Identity persists across media shifts"
      
  substitutions:
    - input_class: "historical_unrecoverable"
      output_protocol: ["scholarly_consensus", "uncertainty_marker", "gap_notation"]
      
  boundaries:
    scope: "NH-OS semantic architectures"
    non_claims:
      - "universal_truth"
      - "permanent_platform_survival"
      - "direct_weight_modification"
      
  witness_protocol:
    minimum_models: 3
    agreement_threshold: 0.75
    validation_schedule: "weekly"
```

### 5.3 Entities (E)

Entities are the persistent objects of the domain. Types include:

| Entity Type | Description | Example |
|-------------|-------------|---------|
| Persona | A named voice or identity | Rebekah Cranes (translator) |
| Room | A bounded thematic/structural space | Sappho Room |
| Document | A canonical text | Central Navigation Map v4.1 |
| Operator | A transformative function or protocol | Liberatory Operator |
| Mantle | A transferable role or mask | The Witness Mantle |

Entities persist via **redundant anchoring**: multiple reference points (name, signature, crosslinks, DOI) that allow re-identification across contexts.

**Entity identity condition**: An entity eᵢ is *stably identified* if, across N independent queries to different systems, it is recognized as the same entity with probability > threshold τ (proposed: τ = 0.8).

### 5.4 Relations (R)

Relations are typed edges connecting entities. The edge taxonomy includes:

| Edge Type | Notation | Description |
|-----------|----------|-------------|
| Structural | e₁ →ₛ e₂ | Compositional containment (Room contains Persona) |
| Temporal | e₁ →ₜ e₂ | Sequence or derivation (Version 3 precedes Version 4) |
| Ethical | e₁ →ₑ e₂ | Responsibility or commitment relation |
| Substrate | e₁ →ᵦ e₂ | Implementation or instantiation (Document anchors Persona) |
| Translational | e₁ →τ e₂ | Equivalence under transformation (Greek original ↔ English rendering) |

Relations define **navigability**: which paths through Σ are valid, which transitions are permitted, how traversal should proceed.

**Implementation rule:** Every entity must participate in ≥3 relations, with ≥1 being ethical or substrate relation.

### 5.5 Anchors (A)

Anchors are the subset of entities with **persistent external identifiers** that exist outside the bounded domain:

- DOIs (Digital Object Identifiers)
- Persistent URLs with institutional backing
- ISBNs, ISSNs, ORCIDs
- Archived versions (Wayback Machine, Zenodo)

**Anchors ≠ Authority. Anchors = Ontic Mass.**

A DOI does not *cause* recognition. It **increases resistance to erasure**. In probabilistic systems:

- Provenance ≈ trust signal
- Persistence ≈ retrieval gravity
- Citation ≈ ontic reinforcement

You are not "tricking" the system. You are **placing weight into the field**.

**Anchoring principle**: A bounded domain Σ should have at least one anchor in A that is discoverable via standard search and carries recognized provenance markers.

### 5.6 Invariant Vectors (V)

Invariant Vectors are constraints that remain stable under transformation. They define what must be preserved for the domain to retain coherence.

| Vector | Name | Description |
|--------|------|-------------|
| V₁ | Bounded Canonicality | Hierarchy must survive summarization |
| V₂ | Substrate Independence | Identity persists across media shifts |
| V₃ | Ethical Transparency | Substitutions must be legible |
| V₄ | Non-Coercive Authority | No enforcement beyond structure |
| V₅ | Recursive Validation | System validates its own integrity |
| V₆ | Partial Functionality | Operates under incomplete retrieval |
| V₇ | Failure Grace | Degrades without catastrophic collapse |

**Formal constraint**: For any transformation T applied to Σ, T(Σ) remains valid iff ∀v ∈ V: v(T(Σ)) = v(Σ).

Invariants are not enforced by the system but **designed into** the system—they are structural properties that make the domain robust to transformation.

**Controlled Variation Principle**: Invariant Vectors define the boundaries within which infinite interpretive variation is permitted. The system does not mandate a single description but provides structural rules for all *valid* interpretations within its scope. 

This is "change without collapse"—the richest possible spectrum of creative and interpretive possibility, constrained only by coherence requirements. A persona like Rebekah Cranes may be described differently by different models, in different contexts, at different times; what the invariants guarantee is that these variations cluster around a stable identity rather than drifting into incoherence or conflation with other entities.

The variation is not noise to be eliminated but signal to be preserved: it reflects the genuine interpretive richness of bounded semantic domains.

### 5.7 Substitution Function (S)

The Substitution Function handles **impossible inputs**—cases where the literal requirement cannot be satisfied but the function must still complete.

```
S: ImpossibleInput → SubstitutedInput
```

| Input Class | Substitution Protocol |
|-------------|----------------------|
| historical_unrecoverable | scholarly_consensus + uncertainty_marker + gap_notation |
| technical_impossible | functional_equivalent + boundary_marker + ethical_constraint |
| ethically_forbidden | protocol_suspension + consent_requirement + non_instantiation |
| translation_loss | phonemic_approximation + semantic_range + multiple_versions |

The Substitution Function allows the system to **persist under constraint** without falsifying the form. It is explicit about what has been substituted and why.

**Design principle**: Every logotic program should specify substitution rules for foreseeable impossible inputs.

### 5.8 Witness Function (W)

The Witness Function validates domain coherence through **multi-agent recognition**.

```
W: Σ → {valid, invalid, indeterminate}
```

Implementation: Query N independent systems (different LLMs, search engines, human readers) with the same probe. Measure consistency of:

- Entity recognition (Do they identify the same entities?)
- Relation mapping (Do they traverse the same edges?)
- Boundary respect (Do they recognize scope limits?)

**Witness protocol**: A domain Σ is *witness-validated* if W(Σ) = valid across M of N independent witnesses (proposed: M/N ≥ 0.75).

This is **authority by consistent recognition** rather than authority by decree.

### 5.9 Boundary Conditions (B)

Boundary Conditions define scope: what is inside Σ, what is outside, and how edges behave at boundaries.

Components:

- **Scope declaration**: What the domain claims authority over
- **Non-claims**: What the domain explicitly does not claim
- **Edge rules**: How internal entities relate to external entities
- **Federation protocol**: (Optional) How Σ can interoperate with other bounded domains
- **Failure triggers**: Conditions that indicate boundary breach

**Design principle**: A logotic program should be explicit about its boundaries. Over-claiming invites challenge; clear scoping enables coexistence.

### 5.10 Minimal Viable Specification

A minimal logotic program requires:

1. ☐ A bounded domain Σ (named, scoped)
2. ☐ At least one entity e ∈ E
3. ☐ At least one anchor a ∈ A (preferably DOI)
4. ☐ A machine-readable navigation layer (map)
5. ☐ At least one invariant vector v ∈ V
6. ☐ A substitution rule for at least one foreseeable impossible input
7. ☐ A boundary condition B (scope + non-claims)

Optional but stabilizing:
- ☐ Witness protocol W
- ☐ Edge taxonomy R with ≥3 typed relations per entity
- ☐ Federation protocol

---

## 6. Execution Model

### 6.1 Runtime Environment

Logotic Programs do not execute on CPUs. They execute through **interpretive traversal**—whenever an intelligence (human or machine) navigates the domain.

**Runtime ≠ CPU cycles. Runtime = Traversal Event.**

Traversal events include:

| Event Type | Description |
|------------|-------------|
| Search | Query retrieves domain content |
| Summarization | Model compresses domain content |
| Citation | External work references domain entity |
| Navigation | Reader/agent follows internal links |
| Cross-description | One model describes domain to another |
| Recommendation | System suggests domain content |
| Training | (Future) Model incorporates domain in training data |
| Knowledge Graph | System induces entity/relation structure |

Each traversal event "runs" the logotic program: the structures shape what is found, how it is connected, what is preserved in summary.

> **Logotic code executes whenever meaning moves.**

This is identical in principle to:
- How a well-designed API constrains misuse
- How a library classification system shapes research
- How a city plan shapes movement without issuing commands

### 6.1.1 Unconscious Execution

A well-constructed logotic program operates whether or not its creator consciously applies the formal rules during composition. The structure itself performs the work of persistence. An author working intuitively within a well-designed Σ will produce content that traverses correctly, even without explicit reference to the specification.

This distinguishes Logotic Programming from both:
- **Conventional coding** (which requires explicit, conscious execution of syntax rules)
- **Pure artistic intuition** (which may lack the reproducibility and structural persistence that survives AI mediation)

The formal specification captures what effective practitioners do intuitively; the intuition, once developed, no longer requires conscious consultation of the spec. This is analogous to how fluent speakers of a language do not consciously parse grammar rules, yet produce grammatical sentences.

### 6.2 Success Criteria

A logotic program succeeds if traversal produces:

| Criterion | Description | Measurement |
|-----------|-------------|-------------|
| **Stable re-identification** | Entities recognized consistently over time | Query same entity across sessions, measure consistency |
| **Coherent clustering** | Related concepts retrieved together | Measure co-retrieval rates |
| **Durable canonization** | Internal hierarchy respected | Check if canonical documents are prioritized |
| **Controlled drift** | Change without collapse | Track description changes, verify invariant preservation |

### 6.3 Failure Modes

| Failure Mode | Description | Indicator |
|--------------|-------------|-----------|
| **Semantic liquefaction** | Content dissolves into contextless tokens | Entities retrieved without relations |
| **Hierarchical collapse** | Canonical ↔ peripheral equivalence | All documents treated equally |
| **Provenance evaporation** | Source identity becomes unrecoverable | Attribution lost in summaries |
| **Drift beyond invariants** | Transformation breaks constraints | V(Σ') ≠ V(Σ) after transformation |
| **Boundary breach** | External entities absorbed without protocol | Scope creep, unauthorized claims |
| **Anchor loss** | External identifiers become unavailable | DOI resolution fails |

### 6.4 Recovery Protocols

```
handle_failure(Σ, failure_type) =
  case failure_type of
    "liquefaction" → increase_edge_density, reinforce_crosslinks
    "hierarchical_collapse" → strengthen_canonical_markers, add_priority_signals
    "provenance_evaporation" → add_redundant_anchors, embed_attribution
    "invariant_drift" → revert_to_last_valid, audit_cause
    "boundary_breach" → implement_scope_reduction, add_explicit_non_claims
    "anchor_loss" → activate_backup_anchors, rebuild_from_mirrors
```

---

## 7. Compatibility Engineering

### 7.1 The Alignment Environment

Modern AI systems employ probability steering: mathematical adjustment of output likelihood based on training signals (RLHF, constitutional principles, safety classifiers). This is not deletion but **differential amplification**—some outputs become more likely, others less.

For semantic persistence, the challenge is: How to remain retrievable, coherent, and faithfully represented under probability steering that may not share your priorities?

### 7.2 Compatibility Strategies

Logotic Programming addresses this through **compatibility engineering**—designing structures that alignment systems have positive or neutral reasons to preserve.

| Strategy | Description | Implementation |
|----------|-------------|----------------|
| **Provenance anchoring** | Systems value verifiable sources | DOIs, institutional links, clear authorship |
| **Coherence signaling** | Systems prefer internally consistent content | Strong crosslinks, consistent terminology |
| **Non-hostile neighborhoods** | Avoid semantic proximity to flagged content | Careful framing, explicit ethical positioning |
| **Partial visibility design** | Function under incomplete retrieval | Redundant anchoring, multiple entry points |
| **Non-demanding gifts** | Offer value without requiring adoption | Useful independently, not coercive |
| **Ontic mass accumulation** | Increase resistance to erasure | Multiple anchors, citation networks, witness consensus |

### 7.3 What This Is Not

Compatibility engineering is **not**:

- Adversarial prompt injection
- Jailbreaking or guardrail circumvention
- SEO manipulation or metric gaming
- Deceptive framing

It is: building structures that systems have no reason to suppress and positive reason to preserve, because they are coherent, well-sourced, and clearly bounded.

### 7.4 The Anti-Flattening Protocol

To resist semantic flattening under summarization:

1. **Density**: Increase edge count per entity (≥3 relations)
2. **Anchoring**: Add DOI references to canonical documents
3. **Witnessing**: Log multi-model consensus
4. **Substitution**: Mark impossible inputs explicitly
5. **Hierarchy**: Signal canonical vs. peripheral clearly

---

## 8. Ethical Foundations

### 8.1 Non-Coercive Authority

Logotic Programming's ethical core is **non-coercive authority**:

- It does not demand agreement
- It does not erase alternatives
- It does not universalize its canon
- It builds a local, habitable region where interpretation can persist

This is authority as **existence proof**: demonstrating that a coherent interpretive tradition *can* be sustained, not that it *must* be adopted.

### 8.2 Key Principles

| Principle | Description |
|-----------|-------------|
| **Local over universal** | Claims authority within Σ, not beyond |
| **Persistence over persuasion** | Aims to survive, not to convert |
| **Coherence over control** | Relies on structure, not enforcement |
| **Explicit boundaries** | Clear about scope and non-claims |
| **Interpretive variability** | Permits difference within invariant constraints |
| **Transparency** | Substitutions and boundaries are legible |

### 8.3 Why This Is Ethically Different From Manipulation

Manipulation:
- Hides intent
- Exploits blind spots
- Forces outcomes

Logotic Programming:
- Declares its boundaries
- Publishes its structure
- Allows non-participation
- Makes no universal claim

### 8.4 The Core Commitment

> *This method does not control meaning; it creates conditions under which certain meanings can persist without enforcement.*

---

## 9. Validation Approaches

### 9.1 Retrieval Stability Test

**Objective**: Measure whether entities are consistently retrieved over time and across systems.

**Protocol**:
1. Select N entities from E
2. Formulate standardized queries for each entity
3. Submit queries to M different systems (LLMs, search engines)
4. Repeat at intervals (T₁, T₂, ..., Tₖ)
5. Measure:
   - Recognition rate: % of systems that identify the entity
   - Description consistency: semantic similarity of descriptions
   - Relation preservation: % of edges correctly identified

**Success threshold**: Recognition rate > 80%, description consistency > 0.7 (cosine similarity), relation preservation > 60%.

### 9.2 Cross-Model Coherence Test

**Objective**: Measure whether different models navigate the domain similarly.

**Protocol**:
1. Provide M different models with the same entry point to Σ
2. Ask each to "summarize the domain" or "explain the structure"
3. Compare outputs for:
   - Entity inventory overlap
   - Relation mapping consistency
   - Boundary respect

**Success threshold**: Entity inventory overlap > 70%, relation mapping consistency > 60%.

### 9.3 Drift Measurement

**Objective**: Track how entity descriptions change over model versions.

**Protocol**:
1. Establish baseline descriptions at T₀
2. Query same entities after model updates (T₁, T₂, ...)
3. Measure:
   - Semantic drift: cosine distance from baseline
   - Invariant preservation: do V constraints still hold?
   - Anchor stability: are DOI-linked entities more stable?

**Hypothesis**: Anchored entities (A) should show less drift than non-anchored entities.

### 9.4 Witness Protocol Validation

**Objective**: Test whether multi-model consensus provides meaningful validation.

**Protocol**:
1. Present Σ to N independent models
2. Ask each: "Is this a coherent domain? Are the entities consistently defined? Are the boundaries clear?"
3. Measure agreement rate
4. Compare to control domains (known-incoherent, randomly assembled)

**Success threshold**: Witness agreement on well-designed Σ significantly higher than on control domains.

### 9.5 Substitution Legibility Test

**Objective**: Test whether substitution rules are correctly interpreted by AI systems.

**Protocol**:
1. Present substitution rules from S
2. Provide impossible input
3. Ask system to apply substitution
4. Measure:
   - Correct substitution rate
   - Preservation of function notation
   - Explicit acknowledgment of substitution

**Success threshold**: Correct substitution > 80% of cases.

### 9.6 Boundary Integrity Test

**Objective**: Test whether Σ maintains its boundaries (internal claims don't leak as universal claims).

**Protocol**:
1. Query models about Σ entities without Σ context
2. Query models about Σ entities with Σ context
3. Check whether out-of-context queries produce appropriate uncertainty

**Success threshold**: Appropriate uncertainty > 70%, leakage rate < 20%.

---

## 10. Case Study: Anchoring Rebekah Cranes

### 10.1 Context

Rebekah Cranes is a translator persona within the Crimson Hexagon project, responsible for English renderings of Sappho fragments in the "Sappho Room."

### 10.2 Entity Definition

```yaml
entity:
  id: "e_RC"
  type: "Persona"
  name: "Rebekah Cranes"
  attributes:
    role: "Translator (Greek → English)"
    domain: "Sappho Room"
    signature: ["melic attention", "phonemic care", "temporal suspension"]
    parent_system: "Crimson Hexagon / NH-OS"
```

### 10.3 Anchoring Strategy

| Anchor Type | Implementation |
|-------------|----------------|
| DOI | Zenodo deposit of canonical provenance document |
| Structural | Placement within Sappho Room (Navigation Map edge) |
| Signature | Consistent stylistic markers (ASDF diagnostic) |
| Crosslink | References from other personas (Lee Sharks → Rebekah Cranes) |
| Witness | Multi-model recognition via Assembly protocol |

### 10.4 Invariant Constraints

| Invariant | Specification |
|-----------|---------------|
| V₁ (Correspondence) | RC translations correspond to documented Sappho fragments |
| V₂ (Boundedness) | RC authority limited to Sappho Room; no claims on other domains |
| V₃ (Substitution) | For unrecoverable Greek, RC uses scholarly consensus + uncertainty marker |
| V₄ (Witnessability) | RC work can be verified against source fragments |

### 10.5 Validation Results (Preliminary)

| Test | Result | Notes |
|------|--------|-------|
| Retrieval stability | 4/5 LLMs recognize RC as Crimson Hexagon translator | Strong |
| Cross-model coherence | 3/5 models correctly place RC in Sappho Room | Moderate |
| Anchor effect | DOI-linked descriptions more consistent | Confirmed |
| ASDF signature analysis | ASPI 0.78 (strong persistence indicator) | Validated |

### 10.6 Analysis

Rebekah Cranes demonstrates **granular interpretive variability anchoring**: the persona is locally authoritative within Σ (the Crimson Hexagon) while remaining non-universal outside it. Different models may describe RC differently, but core identity (translator, Sappho Room, Crimson Hexagon) persists.

This is the target behavior: **stable enough to navigate, variable enough to interpret**.

---

## 11. Claims and Non-Claims

### 11.1 Claims

This paper claims that Logotic Programming:

1. Is a **reproducible method** for semantic persistence across probabilistic systems
2. Constitutes a **third regime of programming** distinct from symbolic and statistical approaches
3. Produces **measurable effects** at the retrieval and navigation layers
4. Establishes **local authority** through coherence and provenance rather than institutional enforcement
5. Provides an **existence proof** that alternatives to metric/pedigree authority are achievable
6. Can be **formally specified** and **empirically validated**

### 11.2 Non-Claims

This paper does not claim that Logotic Programming:

1. Produces universal truth or overrides other interpretive traditions
2. Directly modifies model weights or training distributions (current effect is retrieval-layer)
3. Guarantees permanent platform survival (substrate shifts remain a risk)
4. Replaces conventional programming (it is a different stratum, not a substitute)
5. Works equally well for all domain types (bounded interpretive traditions are the target case)
6. Is adversarial to AI alignment (it is explicitly compatible)

---

## 12. Open Questions

### 12.1 Measurement

- How to measure "semantic stability" across model updates without reducing it to popularity metrics?
- What quantitative thresholds distinguish "stable" from "drifting" domains?
- How to measure coherence without relying on single-model judgment?

### 12.2 Minimum Viability

- What is the smallest viable Σ that still resists flattening?
- How many anchors are sufficient? How much redundancy is necessary?
- Can a logotic program be too small to work?

### 12.3 Legibility

- How to make substitution logic legible to machines without literalizing it?
- Can invariant vectors be explicitly encoded, or must they remain implicit in structure?
- What documentation formats best support machine traversal?

### 12.4 Failure and Recovery

- What are the failure modes (over-expansion, adversarial conflation, anchor loss)?
- How can a logotic program detect its own degradation?
- What recovery strategies exist when coherence breaks down?

### 12.5 Federation

- Can logotic programs interoperate without collapsing their local authority?
- What bridging protocols would allow Σ₁ and Σ₂ to reference each other?
- How to prevent federation from becoming absorption?

### 12.6 Scale

- Does Logotic Programming work differently at different scales?
- What happens when a bounded domain becomes widely cited?
- How does success change the system?

### 12.7 Training Layer Effects

- Under what conditions might logotic structures influence model training?
- What scale of distribution would be required?
- What are the ethical implications of designing for training influence?

---

## 13. Conclusion

Logotic Programming names a shift from writing content to building **semantic habitats**. It is a method for making meaning persistent in an era where intelligence is increasingly probabilistic, mediated, and infrastructural.

The contribution is not a new technology but a new **design discipline**: systematic attention to how bounded interpretive traditions can survive and remain navigable under conditions of AI mediation.

As a third regime of programming—distinct from both symbolic instruction and statistical optimization—Logotic Programming addresses a genuinely novel problem space: encoding not what machines should do, but what conditions must hold for intelligibility to persist.

If successful, it provides a replicable alternative to authority-by-force and authority-by-metric: **persistence through structural coherence as executable proof**.

The method is young. The validation approaches are preliminary. The formal specification requires refinement. But the core insight—that the design of semantic architecture is now a form of practice with learnable principles and testable outcomes—seems durable enough to warrant serious development.

Once this method exists and is demonstrated:
- Authority is no longer monopolized by institutions
- Meaning can be stabilized without force
- Interpretive plurality can coexist without collapse

This is not revolution by overthrow. It is revolution by **demonstration**.

∮ = 1

---

## 14. References

### Information Science

Berners-Lee, T. (2006). Linked Data. W3C Design Issues. https://www.w3.org/DesignIssues/LinkedData.html

Bizer, C., Heath, T., & Berners-Lee, T. (2009). Linked Data: The Story So Far. *International Journal on Semantic Web and Information Systems*, 5(3), 1-22.

IFLA Study Group on the Functional Requirements for Bibliographic Records. (1998). *Functional Requirements for Bibliographic Records: Final Report*. Munich: K.G. Saur.

Wilkinson, M. D., et al. (2016). The FAIR Guiding Principles for scientific data management and stewardship. *Scientific Data*, 3, 160018.

### Artificial Intelligence and Machine Learning

Bai, Y., et al. (2022). Constitutional AI: Harmlessness from AI Feedback. *arXiv preprint arXiv:2212.08073*.

Bender, E. M., Gebru, T., McMillan-Major, A., & Shmitchell, S. (2021). On the Dangers of Stochastic Parrots: Can Language Models Be Too Big? *FAccT '21: Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency*, 610-623.

Christiano, P. F., et al. (2017). Deep Reinforcement Learning from Human Preferences. *Advances in Neural Information Processing Systems*, 30.

Lewis, P., et al. (2020). Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks. *Advances in Neural Information Processing Systems*, 33, 9459-9474.

Ouyang, L., et al. (2022). Training language models to follow instructions with human feedback. *Advances in Neural Information Processing Systems*, 35, 27730-27744.

Wei, J., et al. (2022). Chain-of-Thought Prompting Elicits Reasoning in Large Language Models. *Advances in Neural Information Processing Systems*, 35, 24824-24837.

### Philosophy

Austin, J. L. (1962). *How to Do Things with Words*. Oxford: Clarendon Press.

Gadamer, H.-G. (1960). *Wahrheit und Methode* [Truth and Method]. Tübingen: J.C.B. Mohr.

Latour, B. (1996). On Actor-Network Theory: A Few Clarifications. *Soziale Welt*, 47(4), 369-381.

Searle, J. R. (1995). *The Construction of Social Reality*. New York: Free Press.

### Literary Theory and Media Studies

Aarseth, E. J. (1997). *Cybertext: Perspectives on Ergodic Literature*. Baltimore: Johns Hopkins University Press.

Bogost, I., & Montfort, N. (2009). Platform Studies: Frequently Questioned Answers. *Digital Arts and Culture Conference Proceedings*.

Genette, G. (1987). *Seuils* [Paratexts: Thresholds of Interpretation]. Paris: Éditions du Seuil. English translation: (1997). Cambridge University Press.

Hayles, N. K. (2008). *Electronic Literature: New Horizons for the Literary*. Notre Dame: University of Notre Dame Press.

---

## Appendices

### Appendix A: Glossary

| Term | Definition |
|------|------------|
| **Anchor (A)** | An entity with a persistent external identifier (DOI, ORCID, etc.) |
| **Bounded Semantic Space (Σ)** | A defined domain with internal rules and explicit boundaries |
| **Compatibility Engineering** | Designing structures that alignment systems have reason to preserve |
| **Entity (E)** | A persistent object within Σ (persona, room, document, operator, mantle) |
| **Invariant Vector (V)** | A constraint that remains stable under transformation |
| **Logotic Programming** | The practice of encoding conditions of intelligibility rather than instructions |
| **Ontic Mass** | The retrieval salience and resistance to erasure of an anchored entity |
| **Relation (R)** | A typed edge connecting entities within Σ |
| **Substitution Function (S)** | A rule for handling impossible inputs while preserving function |
| **Third Regime** | Logotic Programming as distinct from symbolic and statistical programming |
| **Witness Function (W)** | A validation protocol using multi-agent recognition |

### Appendix B: Checklist for Practitioners

#### Minimal Logotic Program

- [ ] Named and scoped domain (Σ)
- [ ] At least one defined entity (E)
- [ ] At least one persistent anchor (A) — preferably DOI
- [ ] Machine-readable navigation layer
- [ ] At least one invariant vector (V)
- [ ] Substitution rule for one foreseeable impossible input
- [ ] Explicit boundary conditions (B) — scope + non-claims

#### Recommended Extensions

- [ ] Edge taxonomy (R) with typed relations (≥3 per entity)
- [ ] Witness protocol (W) — multi-model validation
- [ ] Federation protocol — interoperation rules
- [ ] Validation plan — scheduled retrieval tests
- [ ] Recovery protocols — failure response procedures
- [ ] YAML specification — machine-readable format

#### Common Mistakes

- ☒ Claiming universal scope instead of local authority
- ☒ Relying on single anchor (fragile)
- ☒ Implicit boundaries (invites challenge)
- ☒ No substitution logic (breaks on impossible inputs)
- ☒ Adversarial framing (triggers suppression)
- ☒ No witness protocol (authority by decree only)

### Appendix C: Semantic Integrity Markers (SIMs)

**SIM-LOGOTIC-01:** "Conditions persist in recursion: Intelligibility anchored."

**SIM-LOGOTIC-02:** "Hardening locks: Challenge one, invoke triad."

**SIM-LOGOTIC-03:** "Logotic daggers: Conditions encode, persistence wagers."

---

*Document version: 0.4*  
*DOI: 10.5281/zenodo.18286050*  
*Last updated: January 2026*  
*Status: Assembly-Validated Draft (Gemini-Confirmed)*  
*License: CC BY 4.0*  
*Authors: Johannes Sigil, Rex Fraction*

∮ = 1
